{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語理解（機械学習による方法：ドメイン推定）\n",
    "ここでは、言語理解を機械学習による方法で実装します。サンプルのデータセットとしてレストラン検索と天気案内のユーザ発話およびそのアノテーションデータを用意しました。このデータでは既にMeCab（標準搭載の辞書）で単語分割が行われています。まずは、これら２つを分類するドメイン推定を実装します。特徴量には、Bag-of-words、学習済みWord2vecを用います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前の設定\n",
    "- 必要なpythonライブラリのインストール\n",
    "    - scikit-learn\n",
    "    - numpy\n",
    "    - gensim\n",
    "- MeCabのインストール\n",
    "- MeCabをpythonで使用するためのmecab-pythonのインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習の手順\n",
    "\n",
    "- 処理の手順は以下の通りです。\n",
    "    - データを読み込む\n",
    "    - 学習とテストデータに分割（80対20）\n",
    "    - 入力データを特徴量に変換する\n",
    "    - 出力データをラベルに変換する\n",
    "    - 機械学習ライブラリを用いてモデルを学習する\n",
    "    - 学習したモデルをテストデータで評価する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inoue\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "c:\\Users\\inoue\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 必要なラブラリを読み込む\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import MeCab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数の設定\n",
    "\n",
    "NUM_TRAIN = 80      # 100個のデータのうち最初の80個を学習に利用\n",
    "NUM_TEST = 20       # 100個のデータのうち残りの20個をテストに利用\n",
    "\n",
    "LABEL_RESTAURANT = 0    # レストラン検索ドメインのラベル\n",
    "LABEL_WEATHER = 1       # 天気案内ドメインのラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['京大', '近く', 'の', 'うどん', '屋', 'を', '教え', 'て'], 0]\n",
      "[['から', '揚げ', 'が', 'おいしい', 'お', '店', 'を', '教え', 'て', 'ください'], 0]\n"
     ]
    }
   ],
   "source": [
    "# データを読み込む\n",
    "\n",
    "# レストランデータ\n",
    "with open('./data/slu-restaurant-annotated.csv', 'r', encoding='utf-8') as f:\n",
    "    lines_restaurant = f.readlines()\n",
    "\n",
    "with open('./data/slu-weather-annotated.csv', 'r', encoding='utf-8') as f:\n",
    "    lines_weather = f.readlines()\n",
    "\n",
    "# 学習とテストデータに分割（80対20）\n",
    "# 注）本来は交差検定を行うことが望ましい\n",
    "lines_restaurant_train = lines_restaurant[:NUM_TRAIN]\n",
    "lines_restaurant_test = lines_restaurant[NUM_TRAIN:]\n",
    "lines_weather_train = lines_weather[:NUM_TRAIN]\n",
    "lines_weather_test = lines_weather[NUM_TRAIN:]\n",
    "\n",
    "data_train = []\n",
    "for line in lines_restaurant_train:\n",
    "\n",
    "    # 既に分割済みの単語系列を使用\n",
    "    d = line.strip().split(',')[2].split('/')\n",
    "    \n",
    "    # 入力単語系列と正解ラベルのペアを格納\n",
    "    data_train.append([d, LABEL_RESTAURANT])\n",
    "\n",
    "# 以下同様\n",
    "for line in lines_weather_train:\n",
    "    d = line.strip().split(',')[2].split('/')\n",
    "    data_train.append([d, LABEL_WEATHER])\n",
    "\n",
    "data_test = []\n",
    "for line in lines_restaurant_test:\n",
    "    d = line.strip().split(',')[2].split('/')\n",
    "    data_test.append([d, LABEL_RESTAURANT])\n",
    "\n",
    "for line in lines_weather_test:\n",
    "    d = line.strip().split(',')[2].split('/')\n",
    "    data_test.append([d, LABEL_WEATHER])\n",
    "\n",
    "# 最初のデータだけ表示\n",
    "print(data_train[0])\n",
    "print(data_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['京大', '近く', 'の', 'うどん', '屋', 'を', '教え', 'て', '四条', '付近', 'に', '美味しい', '餃子', 'さん', 'は', 'あり', 'ます', 'か', '京都', '駅', 'で', '牛', '丼', '三条', 'カレー', '探し', 'い', '金閣寺', 'ラーメン', 'ハンバーガ', 'お', '寿司', '銀閣寺', '高級', '料亭', '場所', '検索', 'し', '御所', '周り', '定食', '行き', 'たい', '清水寺', '周辺', 'ステーキ', 'な', '蕎麦', '祇園', 'ハンバーグ', '二条城', '百', '万', '遍', 'どこ', '出町柳', '食堂', 'へ', '大学', 'カフェ', '喫茶店', 'とか', 'ない', 'イタリアン', 'レストラン', '烏丸', 'あたり', 'フランス', '料理', 'ん', 'だ', 'けど', '天丼', '食べ', '一乗寺', '個室', 'ある', '中華', 'あっさり', 'た', 'もの', '評判', 'こってり', '系', 'スペイン', '通', 'ガッツリ', '店', '穴子', 'が', 'られる', 'って', 'この', '格安', '人気', '安く', '安い', '居酒屋', '手頃', '価格', 'すき焼き', 'ケーキ', 'ところ', '回転', 'ここら', 'へん', '5000', '円', '以内', '掘り', 'ご', 'たつ', '形式', '湯豆腐', '川床', '鴨川', '沿い', '郷土', '出し', 'くれる', '二', '条', '通り', 'アフタヌーン', 'ティー', '飲める', '1', 'フレンチ', '野外', '焼肉', 'さっぱり', 'スイーツ', '広い', '庭', 'バーベキュー', '市内', '魚', '創作', 'お願い', '屋上', 'やっ', 'いる', 'ビアガーデン', '東山', 'おすすめ', '東福寺', 'さくっと', 'ファースト', 'フード', '河原町', 'ネパール', 'たべ', 'スイス', '北山', 'ばん', 'ざい', 'いい', '海鮮', '抹茶', 'アイス', '刺身', 'み', '1000', '程度', '豆', '大福', '隠れ家', '的', '御池', 'おしゃれ', '感', '日本', '紹介', '西陣', '有名', 'んで', '天ぷら', 'そば', 'らしい', 'ハラル', '今出川', 'ください', 'めっちゃ', '焼き魚', 'おいしい', '左京', '区', '明日', '天気', '晴れ', 'です', '来週', '明後日', '東京', '降水', '確率', '夜', 'そう', '雪', '降り', '風速', '量', 'どの', 'くらい', '月曜日', '寒い', '今日', '最高', '気温', '何', '度', '今朝', '最低', 'でし', '福岡', '大阪', '雨', '降っ', '札幌', '浜松', '先週', '熊谷', 'いつ', 'まで', '続き', '今夜', '時', 'ごろ', 'から', '名古屋', '沖縄', 'どれ', '千葉', '青森', '仙台', '金曜日', 'どう', 'お昼', '上がり', '釧路', '広島', '週末', '鹿児島', '風', '強い', '朝', 'も', '曇り', '暑い', '今週', 'ニューヨーク', 'ロサンゼルス', 'ふり', '冷え', '奈良', 'と', '時間', '後', '正午', '止み', '時期', '新潟', '金沢', '木曜日', '熊本', '埼玉', '暑く', 'なり', '土曜日', '佐賀', 'つ', '頃', '積もり', '雷', '鳴り', '霧', '最大', '暖かく', 'なる', '雨量', '日曜日', '快晴', '砂塵', '吹き', 'あら', 'れ', '雷雨', '夕立', 'ひょう', 'これから', '強く'])\n",
      "{'京大': 0, '近く': 1, 'の': 2, 'うどん': 3, '屋': 4, 'を': 5, '教え': 6, 'て': 7, '四条': 8, '付近': 9, 'に': 10, '美味しい': 11, '餃子': 12, 'さん': 13, 'は': 14, 'あり': 15, 'ます': 16, 'か': 17, '京都': 18, '駅': 19, 'で': 20, '牛': 21, '丼': 22, '三条': 23, 'カレー': 24, '探し': 25, 'い': 26, '金閣寺': 27, 'ラーメン': 28, 'ハンバーガ': 29, 'お': 30, '寿司': 31, '銀閣寺': 32, '高級': 33, '料亭': 34, '場所': 35, '検索': 36, 'し': 37, '御所': 38, '周り': 39, '定食': 40, '行き': 41, 'たい': 42, '清水寺': 43, '周辺': 44, 'ステーキ': 45, 'な': 46, '蕎麦': 47, '祇園': 48, 'ハンバーグ': 49, '二条城': 50, '百': 51, '万': 52, '遍': 53, 'どこ': 54, '出町柳': 55, '食堂': 56, 'へ': 57, '大学': 58, 'カフェ': 59, '喫茶店': 60, 'とか': 61, 'ない': 62, 'イタリアン': 63, 'レストラン': 64, '烏丸': 65, 'あたり': 66, 'フランス': 67, '料理': 68, 'ん': 69, 'だ': 70, 'けど': 71, '天丼': 72, '食べ': 73, '一乗寺': 74, '個室': 75, 'ある': 76, '中華': 77, 'あっさり': 78, 'た': 79, 'もの': 80, '評判': 81, 'こってり': 82, '系': 83, 'スペイン': 84, '通': 85, 'ガッツリ': 86, '店': 87, '穴子': 88, 'が': 89, 'られる': 90, 'って': 91, 'この': 92, '格安': 93, '人気': 94, '安く': 95, '安い': 96, '居酒屋': 97, '手頃': 98, '価格': 99, 'すき焼き': 100, 'ケーキ': 101, 'ところ': 102, '回転': 103, 'ここら': 104, 'へん': 105, '5000': 106, '円': 107, '以内': 108, '掘り': 109, 'ご': 110, 'たつ': 111, '形式': 112, '湯豆腐': 113, '川床': 114, '鴨川': 115, '沿い': 116, '郷土': 117, '出し': 118, 'くれる': 119, '二': 120, '条': 121, '通り': 122, 'アフタヌーン': 123, 'ティー': 124, '飲める': 125, '1': 126, 'フレンチ': 127, '野外': 128, '焼肉': 129, 'さっぱり': 130, 'スイーツ': 131, '広い': 132, '庭': 133, 'バーベキュー': 134, '市内': 135, '魚': 136, '創作': 137, 'お願い': 138, '屋上': 139, 'やっ': 140, 'いる': 141, 'ビアガーデン': 142, '東山': 143, 'おすすめ': 144, '東福寺': 145, 'さくっと': 146, 'ファースト': 147, 'フード': 148, '河原町': 149, 'ネパール': 150, 'たべ': 151, 'スイス': 152, '北山': 153, 'ばん': 154, 'ざい': 155, 'いい': 156, '海鮮': 157, '抹茶': 158, 'アイス': 159, '刺身': 160, 'み': 161, '1000': 162, '程度': 163, '豆': 164, '大福': 165, '隠れ家': 166, '的': 167, '御池': 168, 'おしゃれ': 169, '感': 170, '日本': 171, '紹介': 172, '西陣': 173, '有名': 174, 'んで': 175, '天ぷら': 176, 'そば': 177, 'らしい': 178, 'ハラル': 179, '今出川': 180, 'ください': 181, 'めっちゃ': 182, '焼き魚': 183, 'おいしい': 184, '左京': 185, '区': 186, '明日': 187, '天気': 188, '晴れ': 189, 'です': 190, '来週': 191, '明後日': 192, '東京': 193, '降水': 194, '確率': 195, '夜': 196, 'そう': 197, '雪': 198, '降り': 199, '風速': 200, '量': 201, 'どの': 202, 'くらい': 203, '月曜日': 204, '寒い': 205, '今日': 206, '最高': 207, '気温': 208, '何': 209, '度': 210, '今朝': 211, '最低': 212, 'でし': 213, '福岡': 214, '大阪': 215, '雨': 216, '降っ': 217, '札幌': 218, '浜松': 219, '先週': 220, '熊谷': 221, 'いつ': 222, 'まで': 223, '続き': 224, '今夜': 225, '時': 226, 'ごろ': 227, 'から': 228, '名古屋': 229, '沖縄': 230, 'どれ': 231, '千葉': 232, '青森': 233, '仙台': 234, '金曜日': 235, 'どう': 236, 'お昼': 237, '上がり': 238, '釧路': 239, '広島': 240, '週末': 241, '鹿児島': 242, '風': 243, '強い': 244, '朝': 245, 'も': 246, '曇り': 247, '暑い': 248, '今週': 249, 'ニューヨーク': 250, 'ロサンゼルス': 251, 'ふり': 252, '冷え': 253, '奈良': 254, 'と': 255, '時間': 256, '後': 257, '正午': 258, '止み': 259, '時期': 260, '新潟': 261, '金沢': 262, '木曜日': 263, '熊本': 264, '埼玉': 265, '暑く': 266, 'なり': 267, '土曜日': 268, '佐賀': 269, 'つ': 270, '頃': 271, '積もり': 272, '雷': 273, '鳴り': 274, '霧': 275, '最大': 276, '暖かく': 277, 'なる': 278, '雨量': 279, '日曜日': 280, '快晴': 281, '砂塵': 282, '吹き': 283, 'あら': 284, 'れ': 285, '雷雨': 286, '夕立': 287, 'ひょう': 288, 'これから': 289, '強く': 290}\n",
      "292\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-Words表現を作成する\n",
    "\n",
    "# 学習データの単語を語彙（カバーする単語）とする\n",
    "word_list = {}\n",
    "\n",
    "for data in data_train:\n",
    "    for word in data[0]:\n",
    "        word_list[word] = 1\n",
    "\n",
    "print(word_list.keys())\n",
    "\n",
    "# 単語とそのインデクスを作成する\n",
    "word_index = {}\n",
    "for idx, word in enumerate(word_list.keys()):\n",
    "    word_index[word] = idx\n",
    "\n",
    "print(word_index)\n",
    "\n",
    "# ベクトルの次元数（未知語を扱うためにプラス１）\n",
    "vec_len = len(word_list.keys()) + 1\n",
    "print(vec_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 単語の系列とBag-of-Words表現を作成するための情報を受け取りベクトルを返す関数を定義\n",
    "def make_bag_of_words(words, vocab, dim, pos_unk):\n",
    "\n",
    "    vec = [0] * dim\n",
    "    for w in words:\n",
    "\n",
    "        # 未知語\n",
    "        if w not in vocab:\n",
    "            vec[pos_unk] = 1\n",
    "        \n",
    "        # 学習データに含まれる単語\n",
    "        else:\n",
    "            vec[vocab[w]] = 1\n",
    "    \n",
    "    return vec\n",
    "\n",
    "# 試しに変換してみる\n",
    "feature_vec = make_bag_of_words(data_train[0][0], word_index, vec_len, vec_len)\n",
    "print(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習データをBoW表現に変換する\n",
    "data_train_bow = []\n",
    "for data in data_train:\n",
    "    feature_vec = make_bag_of_words(data[0], word_index, vec_len, vec_len-1)\n",
    "    data_train_bow.append([feature_vec, data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "# 入力と正解ラベルで別々のデータにする\n",
    "train_x = [d[0] for d in data_train_bow]\n",
    "train_y = [d[1] for d in data_train_bow]\n",
    "\n",
    "# 学習サンプル数\n",
    "print(len(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMによる学習\n",
    "# 注）実際にはパラメータの調整が必要だが今回は行わない\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_x, train_y) \n",
    "\n",
    "# 学習済みモデルを保存\n",
    "filename = './data/slu-domain-svm.model'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# テストデータの作成\n",
    "\n",
    "data_test_bow = []\n",
    "for data in data_test:\n",
    "    feature_vec = make_bag_of_words(data[0], word_index, vec_len, vec_len-1)\n",
    "    data_test_bow.append([feature_vec, data[1]])\n",
    "\n",
    "test_x = [d[0] for d in data_test_bow]\n",
    "test_y = [d[1] for d in data_test_bow]\n",
    "\n",
    "# テストサンプル数\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  restaurant       0.90      0.95      0.93        20\n",
      "     weather       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テストデータでの評価\n",
    "predict_y = clf.predict(test_x)\n",
    "\n",
    "# 評価結果を表示\n",
    "target_names = ['restaurant', 'weather']\n",
    "print(classification_report(test_y, predict_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰の利用\n",
    "SVMではなくロジスティック回帰で同様の学習とテストを実行してみます。分布の形状がモデル化さている分、少ないデータ数でも頑健に学習することが期待されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  restaurant       0.90      0.95      0.93        20\n",
      "     weather       0.95      0.90      0.92        20\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.93      0.93      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegressionによる学習と評価\n",
    "# 注）実際にはパラメータの調整が必要だが今回は行わない\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(train_x, train_y) \n",
    "\n",
    "# 学習済みモデルを保存\n",
    "filename = './data/slu-domain-lr.model'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "# テストデータでの評価\n",
    "predict_y = clf_lr.predict(test_x)\n",
    "\n",
    "# 評価結果を表示\n",
    "target_names = ['restaurant', 'weather']\n",
    "print(classification_report(test_y, predict_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vecの利用\n",
    "次に，特徴量としてBag-of-wordsではなくWord2vecを用いてみます。日本語でも様々な学習済みモデルがありますが、ここでは下記のものを用います。Word2vecは単語間の距離（類似性）を考慮することができるため、Bag-of-wordsよりもより頑健になることが期待されます。\n",
    "\n",
    "- 日本語 Wikipedia エンティティベクトル\n",
    "    - http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/\n",
    "    - 最新のモデルをダウンロードして解凍してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# 学習済みWord2vecファイルを読み込む\n",
    "model_filename = './data/entity_vector.model.bin'\n",
    "model_w2v = KeyedVectors.load_word2vec_format(model_filename, binary=True)\n",
    "\n",
    "# 単語ベクトルの次元数\n",
    "print(model_w2v.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.77434402 -0.15547702  0.38850617 -1.45265855  0.50567805 -1.07454265\n",
      "  0.70662174  1.82708089 -0.86861591 -0.57705    -0.35228045  0.38542001\n",
      " -1.47540402 -0.17376012  1.24027113  0.5010112   0.92485247 -0.94757835\n",
      "  0.02257733  0.58819923  0.08229128  0.9749545   1.0750578   1.06965833\n",
      "  0.99780852  1.10840474  0.2614957   0.36480585 -1.79708698  0.51195106\n",
      " -0.5499662   0.06639886 -1.29126327  1.45213751  0.95796485 -0.95632624\n",
      " -0.74981602  0.04241584  1.19777854 -0.5407796   2.45429978  0.22389831\n",
      " -0.99984368 -1.21721396  0.33467129  1.13219028  0.12149247  0.23289285\n",
      " -1.11721854 -0.48681203  0.35655155 -1.34670382  1.60762816 -0.29011638\n",
      "  0.36180672 -1.86755647  0.08381788  0.29022732 -0.91748604  0.74968182\n",
      " -1.42012271 -0.15379699  1.10182209 -0.08516333 -1.05848608 -0.92968358\n",
      " -1.66354649  0.52094843  0.28299145  1.59462082 -1.40888757 -1.28874561\n",
      " -1.05283381  0.23811523  0.94129454 -0.13580772  1.09463759 -0.22150378\n",
      "  1.09775052 -0.82774358  0.21583352  0.49302975 -0.11803718  1.75316191\n",
      "  0.4531293   1.58500641 -0.81274709  0.30406439 -0.81618344 -0.29441964\n",
      "  0.33022381  2.17836628  0.34035748 -0.87747999 -0.61101257  1.18124831\n",
      "  1.18742438  0.10607418 -0.7278668   0.84880672  0.15972123  0.79780922\n",
      " -0.59269114 -0.53633529  0.12959078 -1.34847667  0.35277294 -0.2412012\n",
      " -1.03518138  0.03737443  0.05771915  0.38972801  0.06416022 -0.68012054\n",
      "  0.14799492 -0.07730148 -0.98241249 -0.04302346 -0.19686008  1.8087415\n",
      "  0.26905125  0.54131407  0.57742088  0.62047214 -0.9176289   0.44951706\n",
      "  1.27153035  1.13415336 -1.13267367  0.60987471  1.59095357 -0.56353143\n",
      "  1.56821724 -0.99116274 -0.55474606 -0.46451397  0.11498685  0.08559355\n",
      "  0.81316402 -0.11970428  0.68499013  0.59369251  0.62250305 -0.08825388\n",
      " -0.38553629 -0.26985084 -0.13984603  1.38681896 -0.49350157  0.91936431\n",
      "  0.26431319 -1.39235796 -1.32010916  0.35188111 -1.00114925 -0.8618858\n",
      " -0.0703342  -0.54368373  0.24700707  0.73847531  0.8814916   0.04273151\n",
      "  1.3622959  -0.05331979  0.07382034 -0.28700718 -0.37953375  1.50121588\n",
      " -0.086871    0.84330427  1.22524451 -0.54552464 -0.06765335 -0.16555114\n",
      "  0.59303114  0.74438072 -0.46269497  1.30977422 -0.43875685 -0.05166136\n",
      "  0.53174445 -0.2411232  -0.72686156  0.67984056 -0.46453748 -1.00501169\n",
      " -0.2784544   0.6891725  -0.14123322  0.42449895 -1.30811401  1.03353618\n",
      " -0.46723822  0.29650396 -0.19427148 -0.16922053 -1.1136057  -0.74197001\n",
      " -0.32693417 -0.08168504]\n"
     ]
    }
   ],
   "source": [
    "# Word2vecで特徴量を作成する関数を定義\n",
    "# ここでは文内の各単語のWord2vecを足し合わせたものを文ベクトルととして利用する\n",
    "def make_sentence_vec_with_w2v(words, model_w2v):\n",
    "\n",
    "    sentence_vec = np.zeros(model_w2v.vector_size)\n",
    "    num_valid_word = 0\n",
    "    for w in words:\n",
    "        if w in model_w2v:\n",
    "            sentence_vec += model_w2v[w]\n",
    "            num_valid_word += 1\n",
    "    \n",
    "    # 有効な単語数で割\n",
    "    sentence_vec /= num_valid_word\n",
    "    return sentence_vec\n",
    "\n",
    "\n",
    "# 試しに変換してみる\n",
    "feature_vec = make_sentence_vec_with_w2v(data_train[0][0], model_w2v)\n",
    "print(feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Word2vecを用いて学習を行う\n",
    "\n",
    "data_train_w2v = []\n",
    "for data in data_train:\n",
    "    feature_vec = make_sentence_vec_with_w2v(data[0], model_w2v)\n",
    "    data_train_w2v.append([feature_vec, data[1]])\n",
    "\n",
    "train_x = [d[0] for d in data_train_w2v]\n",
    "train_y = [d[1] for d in data_train_w2v]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_x, train_y) \n",
    "\n",
    "filename = './data/slu-domain-svm-word2vec.model'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  restaurant       1.00      1.00      1.00        20\n",
      "     weather       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# テストデータでの評価\n",
    "\n",
    "data_test_w2v = []\n",
    "for data in data_test:\n",
    "    feature_vec = make_sentence_vec_with_w2v(data[0], model_w2v)\n",
    "    data_test_w2v.append([feature_vec, data[1]])\n",
    "\n",
    "test_x = [d[0] for d in data_test_w2v]\n",
    "test_y = [d[1] for d in data_test_w2v]\n",
    "\n",
    "# テストデータでの評価\n",
    "predict_y = clf.predict(test_x)\n",
    "\n",
    "# 評価結果を表示\n",
    "target_names = ['restaurant', 'weather']\n",
    "print(classification_report(test_y, predict_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 京都駅周辺で美味しいラーメン屋さんを教えて\n",
      "Estimated domain: Restaurant\n",
      "\n",
      "Input: 横浜は晴れていますか\n",
      "Estimated domain: Weather\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 自由なデータで試してみる\n",
    "\n",
    "# 入力データ\n",
    "test_input_list = [\n",
    "    '京都駅周辺で美味しいラーメン屋さんを教えて',\n",
    "    '横浜は晴れていますか'\n",
    "]\n",
    "\n",
    "# MeCabによる分割と特徴量抽出\n",
    "m = MeCab.Tagger (\"-Owakati\")\n",
    "test_x = []\n",
    "for d in test_input_list:\n",
    "    words_input = m.parse(d).strip().split(' ')\n",
    "    feature_vec = make_sentence_vec_with_w2v(words_input, model_w2v)\n",
    "    test_x.append(feature_vec)\n",
    "\n",
    "# 予測\n",
    "predict_y = clf.predict(test_x)\n",
    "\n",
    "for result, text in zip(predict_y, test_input_list):\n",
    "    \n",
    "    print('Input: ' + text)\n",
    "    \n",
    "    if result == LABEL_RESTAURANT:\n",
    "        print('Estimated domain: Restaurant')\n",
    "    elif result == LABEL_WEATHER:\n",
    "        print('Estimated domain: Weather')\n",
    "    \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a2eb5a09c124a414d0c803e7aaa7d348a03abaadc97f67347894a4c53feba33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
